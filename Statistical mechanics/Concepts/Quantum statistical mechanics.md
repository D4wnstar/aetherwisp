---
wiki-publish: true
---
Statistical mechanics can be extended to quantum mechanics by applying its [[Postulati della meccanica quantistica|principles]] to the preexisting theory. However, due to the complexity of statistical [[Physical system|systems]] and since thermodynamics in general tries to explain phenomena at a high level, some care needs to be take. Real statistical systems are inherently *open systems*, that is, they interact with the environment. The only exceptions are [[Physical system|isolated systems]] described by the [[microcanonical ensemble]], but realistic microcanonical descriptions are few and far between outside of theory. Open systems are notoriously complicated to deal with and many quantum systems are approximated as closed for the sake of simplicity. However, we generally can't do that in statistical mechanics, which attempts to find points of equilibrium with the surrounding environment. In theory, what we need is the [[Hamiltonian]] of the system and the Hamiltonian of the environment, from which we can figure out the coupling. However, the environment Hamiltonian is so complicated, we can't do much with it even if we manage to find it in the first place. We need a way around that.
### Quantum regime
First things first, let's define when we should use quantum mechanics in the first place. Thanks to the [[equipartition theorem]], we know that the [[kinetic energy]] $K$ of a [[Particle|particle]] in a gas depends on the [[temperature]] of the system. For an [[ideal gas]] we have
$$K=\frac{p_{0}^{2}}{2m}=\frac{3}{2}k_{B}T$$
and more generally $K\propto T$. Using the [[Formula di de Broglie|de Broglie wavelength]] we get
$$\lambda_{0}=\frac{h}{\sqrt{ 3mk_{B}T }}$$
which gives us a measure of the spatial extension of a particle. Due to the [[Disuguaglianza di Heisenberg|uncertainty principle]], the uncertainty on spatial extension $\Delta x$ and momentum spread $\Delta p$ are related by $\Delta x\Delta p\sim \hbar$[^1]. The wavelength $\lambda_{0}$ serves as an order-of-magnitude estimate of the spatial extension. A more convenient metric of spatial extent is the [[Formula di de Broglie|de Broglie thermal wavelength]], defined as
$$\lambda=\sqrt{ \frac{2\pi \hbar^{2}}{mk_{B}T} }$$
which is really just a rescaled form of the previous one (it makes for nicer-looking equations, that's all). Quantum effects begin to occur when the [[Funzione d'onda|wavefunction]] superposition starts to crop up. For two wavefunctions to superimpose, they must more or less occupy the same space, and for that to happen, the particles must close enough that their spatial extent is less than their distance. As such, if we take particle density $n$ to be a measure of average interparticle distance (which is proportional to $n^{1/3}$), we can state that quantum effects begin to be relevant when
$$n\lambda ^{3}\simeq 1\quad\text{(onset of quantum effects)}$$
Much more than this, and quantum effects are negligible. Note that $\lambda$ is dependent on temperature. This means that we can rewrite the previous equation to find a specific temperature at which a gas starts to behave like a quantum system. If we use the full equality $n\lambda ^{3}=1$ and extract $T$ out of $\lambda$ we find
$$T_{0}=\left( \frac{2\pi \hbar^{2}}{k_{B}m} \right)n^{2/3}$$
We call $T_{0}$ the [[degeneracy temperature]]. With anything near and especially below this value, we really should take quantum physics into account. The actual temperature changes a lot between materials and systems. For instance, atomic hydrogen ($H_{2}$) gas at particle density of $n=2\times 10^{19}\text{ particles/cm}^{3}$ has a degeneracy temperature of $T_{0}=5\times 10^{-2}\text{ K}$. However, if we express free [[Elettrone|electrons]] in metal as a particle gas of density $n=10^{22}\text{ particles/cm}^{3}$ we get a temperature of $T_{0}=10^{4}\text{ K}$. A difference of six orders of magnitude. Not only that, but this shows that some systems, like the aforementioned free electrons, are deeply quantum-dominated even at room temperature, a fact that differs significantly from the typical cryogenic temperatures we see in most quantum examples and technology.
### General treatment
One of the basic [[Postulati della meccanica quantistica|principles of quantum mechanics]] is that a [[stato|state]] $\ket{\psi}$ can be expressed as a [[Serie di Fourier|Fourier series]] of independent [[Equazione agli autovalori|eigenfunctions]] $\ket{\phi_{n}}$ as
$$\ket{\psi} =\sum_{n}c_{n}\ket{\phi_{n}}  $$
where $c_{n}$ are coefficients. Each eigenfunction represents one possible state of the system and we can describe the actual state as a mixture of these possibilities. In fact, this is just the definition of [[ensemble]], a collection of identical copies of a system in different states, each identified by an eigenfunction. Now, the equilibrium state of the system is given by the quantum [[Hamiltonian]] $\hat{H}$ and the [[Equazione di Schrödinger|Schrödinger equation]]:
$$\hat{H}\ket{\phi_{n}}=E_{n}\ket{\phi_{n}}  $$
 In principle, we are done. We found a sensible definition of a quantum ensemble that follows directly from the basics of quantum mechanics and we can use well-known methods to actually compute quantities. But, if we dig a little deeper, we'll start to get some issues, namely with ensemble averages.

The [[ensemble average]] of an [[Osservabile|observable]] $\hat{O}$ makes a somewhat direct conversion to quantum physics, using the usual definition of the [[mean]] in a certain state $\ket{\psi}$:
$$\langle \hat{O} \rangle =\frac{\braket{ \psi | \hat{O} |\psi }}{\braket{ \psi | \psi } }=\frac{\sum_{n}\sum_{m}c_{n}^{*}c_{m} \braket{ \phi_{n} | \hat{O} |\phi_{m} }}{\sum_{k}c_{k}^{*}c_{k}}$$
since $\braket{ \phi_{n} | \phi_{m} }=\delta_{nm}$ using the [[Kronecker delta|Kronecker delta]]. The coefficients are dependent on time, so they are not constant. However, we are only trying to find the equilibrium state, which by definition is not dependent on time[^2]. To reconcile the two, we can use the time-average of the coefficients $\langle c_{n}^{*}c_{m} \rangle_{t}$ instead:
$$\langle \hat{O} \rangle =\frac{\sum_{n}\sum_{m}\langle c_{n}^{*}c_{m} \rangle_{t} \braket{ \phi_{n} | \hat{O} |\phi_{m} }}{\sum_{n}\langle c_{n}^{*}c_{n} \rangle_{t}}  $$
However, here we hit a wall. This equation is pretty much unsolvable. In order to make it usable, we need to understand what's going on. The reason for complexity is the double sum in the numerator. The sums are done over states (indexed by $n$ and $m$), so all the terms with $n\neq m$ are cross-state terms where separate states can affect each other. This phenomenon is called [[quantum interference]] and has no classical analog, which is why we did not struggle with this in classical ensembles.

The nature of interference is due to quantum quantities being complex numbers instead of reals. To better explain the phenomenon, we can rewrite the coefficients in polar form $c_{n}=r_{n}e^{i\theta_{n}}$. $e^{i\theta_{n}}$ is the [[complex phase]] and the culprit of the problem. Using this in the previous equation yields
$$\langle \hat{O} \rangle =\frac{\sum_{n}\sum_{m}\langle r_{n}^{*}r_{m}e^{i(\theta_{m}-\theta _{n})} \rangle_{t} \braket{ \phi_{n} | \hat{O} |\phi_{m} }}{\sum_{n}\langle c_{n}^{*}c_{n} \rangle_{t}}  $$
All terms with $n\neq m$ have $\theta_{m}-\theta_{n}\neq 0$, which leads to a nonzero phase. For $n=m$, the phase vanishes and we are left with magnitude $r_{n}^{*}r_{m}$ alone. So what do we do?

In actual physical systems, interference is a consequence of [[Indeterminatezza quantistica|quantum indeterminacy]]. Multiple eigenstates are thought to coexist simultaneously, forming the full, mixed state of a quantum object. This mixed state can be described as a Fourier series of eigenstates. Once observed, the mixed state collapses into into a pure state, picking among the component eigenstates with probability determined by the square modulo of the coefficients.

In ensembles however, there is a key difference. The simultaneous coexistence of states *does not happen* because ensembles *do not physically exist*. They are merely a mathematical abstraction and device to calculate things. There cannot possibly be interference between states that only exists in theory. Think of it like this: if you took a snapshot of a free [[Elettrone|electron]], its state would be a superposition of the [[spin]] up and down states. If you took a snapshot of a quantum [[ideal gas]], its state would be just one pure state, no superposition. The ensemble superposition only exists as a mathematical device listing all possible arrangements of the system, but there is no physical substance. Put in other words: quantum superposition occurs due to *physical uncertainty*, ensemble superposition occurs due to *human ignorance regarding the specifics of the system*.

With this clear, we can safely determine that there can be no interference among members of the ensemble. For this to occur at a macroscopic scale, it is sufficient for the phases to be considered [[Random variable|random variables]], such that on (time) average, they tend to cancel each other out, leaving us only with an *incoherent* superposition of pure states. It is this kind of superposition that defines a quantum ensemble. This is called the [[random phase hypothesis]] and we can argue that this randomness occurs due to the massive complexity of the system, especially when into contact with the environment. Mathematically, it is equivalent to the statement
$$\langle c_{n}^{*}c_{m} \rangle_{t}=0\quad\text{for}\quad n\neq m$$
This makes the set of all coefficients $\{ c_{n} \}_{n\in \mathbb{N}}$ [[Orthogonality|orthogonal]] (but not necessarily [[Normalization|normalized]]). However, we can also enforce normalization within the energy eigenstates of our systems because only normalized states are physically valid, so we discard all unnormalized states due to being unphysical. If we call $\langle c_{n}^{*}c_{n} \rangle_{t}=\lvert b_{n} \rvert^{2}$ we get the quantum ensemble average
$$\boxed{\langle \hat{O} \rangle =\frac{\sum_{n}\lvert b_{n} \rvert ^{2} \braket{ \phi_{n} | \hat{O} |\phi_{n} }}{\sum_{n}\lvert b_{n} \rvert ^{2}}  }$$
#### Density matrix
Since interference is removed, the states of the system can be neatly packaged as a [[Matrice di densità|density matrix]] $\hat{\rho}$:
$$\hat{\rho}=\sum_{n}\lvert b_{n} \rvert ^{2}\ket{\phi_{n}}\bra{\phi_{n}} =\sum_{n}\lvert b_{n} \rvert ^{2}\hat{P}_{\phi_{n}}=\begin{pmatrix}
\lvert b_{1}\rvert^{2}\hat{P}_{\phi_{1}} & 0 & \ldots & 0 \\
0 & \lvert b_{2} \rvert^{2}\hat{P}_{\phi_{2}} & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & \lvert b_{N} \rvert ^{2}\hat{P}_{\phi_{N}}
\end{pmatrix}$$
where $\hat{P}_{\phi_{n}}$ is the [[Proiettore|eigenprojector]] over the $n$-th eigenstate. Note that in the $\lvert b_{n} \rvert^{2}$ [[base|basis]], it is diagonal. Like all density matrices, it has the crucial property that its [[Traccia|trace]] is $\text{Tr}(\hat{\rho})=\sum_{n}\lvert b_{n} \rvert^{2}$. We can therefore state:
$$\langle \hat{O} \rangle =\frac{\sum_{n}\braket{ \phi_{n} | \hat{O} \hat{\rho} | \phi_{n} } }{\sum_{n}\braket{ \phi_{n} | \hat{\rho} | \phi_{n} } }=\frac{\sum_{n,m}\braket{ \phi_{n} | \hat{O} | \phi_{m} }\braket{ \phi_{m} | \hat{\rho} | \phi_{n} }  }{\sum_{n}\braket{ \phi_{n} | \hat{\rho} | \phi_{n} } }=\boxed{\frac{\text{Tr}(\hat{O} \hat{\rho})}{\text{Tr}(\hat{\rho})}}$$
This formula is how we derive all ensemble averages in quantum statistical mechanics. As an additional connection, recall that the [[partition function]] of a classical ensemble is just the sum of the density function over all states. In quantum ensembles, it is the sum of the density matrix over all states. But this has a trivial form:
$$Z=\sum_{n}\hat{\rho}_{nn}=\text{Tr}(\hat{\rho})$$
So, perhaps unsurprisingly, we get
$$\langle \hat{O} \rangle =\frac{\text{Tr}(\hat{O}\hat{\rho})}{Z}$$

[^1]: Of course, the product of the uncertainties can go up to infinity, but we care systems with realistically low uncertainty (although not necessarily lowest-uncertainty states).
[^2]: The time it takes for microscopic processes to complete (called the *relaxation time*) is tiny compared to macroscopic time. Microscopic oscillations are so frequent, localized and chaotic that their overall effect vanishes when looking at the whole system over macroscopic time. Averaging should occur a time that is large from a microscopic perspective, but small over a macroscopic one.